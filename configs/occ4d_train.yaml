mode: "train"

##Data
data:
  dataset_name: "nuscenes"  # "kitti", "argo"
  dataset_pct: 100  # not used now; percentage of data used for training
  voxel_size: 0.2  # voxel size
  pc_range: [-70.0, -70.0, -4.5, 70.0, 70.0, 4.5]  # valid point cloud range
  time_interval: 0.05
  n_input: 6  # number of input point cloud scan 6, 9, 6
  n_skip: 4
  n_output: 6  # number of output point cloud scan (for pretraining only)
  input_within_pc_range: True
#  transform: True # If true, the points are pose-aligned before feeding to the model
  ego_mask: True # mask the points of ego-vehicle, 4docc pretraining default=True
  shuffle: False  # shuffle the dataset
  flip: True  # flip the x-axis in Singapore (left hand drive)
  fgbg_label: False  # use foreground background labels to train the model

##Training
model:
  resume_ckpt: # "/root/4DOCC/logs/pretrain/nuscenes/pretrain_vs-0.2_t-3.0_bs-2_epo-60/version_1/checkpoints/pretrain_vs-0.2_t-3.0_bs-2_epo-60_epoch=9.ckpt"
  loss_type: "l1"  # "l1", "l2", "absrel" for pre-training only
  optimizer: "adam"
  lr_start: 0.0001  # 4dmos: 0.0001, 4docc_org: 0.0005
  lr_epoch: 1       # 4dmos: 1       4docc_org: 5
  lr_decay: 0.9     # 4dmos: 0.99    4docc_org: 0.1
  weight_decay:  # not used
  num_epoch: 60
  batch_size: 4
  acc_batches: 1 # accumulate gradients over k batches before stepping into the optimizer
  num_workers: 8
  num_devices: 1 # could be integer of how many cuda devices to use, or a list that specify which devices to use [0, 1, 2, 3]
  # AUGMENTATION: True

# Dataset Config
dataset:
  nuscenes:
    root: "/home/user/Datasets/nuScenes" # "/root/autodl-tmp/datasets/nuscenes" "/home/user/Datasets/nuScenes"
    version: "v1.0-trainval"
  kitti:
    root: "/home/user/Datasets/KITTI"
    config: "configs/semantic-kitti.yaml"
  argo:
    root: "/home/user/Datasets/ArgoVerse2/LiDAR/"
